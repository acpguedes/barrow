{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or later</li> <li>DuckDB and Apache Arrow are required. They are installed automatically when using <code>pip</code> but can also be installed system wide for optimal performance.</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install barrow\n</code></pre> <p>This command installs <code>barrow</code> along with its core dependencies such as <code>pyarrow</code>, <code>duckdb</code> and <code>numpy</code>.</p>"},{"location":"installation/#development-install","title":"Development Install","text":"<p>For contributing or running the test suite, install the optional development and documentation dependencies:</p> <pre><code>pip install -e .[dev,docs]\n</code></pre> <p>This installs linting tools, test runners and documentation tooling like MkDocs.</p>"},{"location":"installation/#using-the-setup-script","title":"Using the setup script","text":"<p>To create an isolated development environment with shell completion, run:</p> <pre><code>source scripts/setup_env.sh\n</code></pre> <p>This creates a virtual environment in <code>.venv</code>, installs development dependencies and enables <code>barrow</code> tab completion for the current shell.</p>"},{"location":"installation/#running-with-docker","title":"Running with Docker","text":"<p>A minimal Dockerfile is provided to run <code>barrow</code> in a container:</p> <pre><code>docker build -t barrow .\ndocker run --rm -it barrow --help\n</code></pre>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, verify that the CLI works:</p> <pre><code>barrow --help\n</code></pre> <p>The command should print the top-level help screen describing available subcommands.</p>"},{"location":"manual/","title":"Manual","text":"<p><code>barrow</code> provides subcommands for working with tabular data. Each command can read from <code>STDIN</code> or a file and write to <code>STDOUT</code> or a file.</p>"},{"location":"manual/#common-io-options","title":"Common I/O options","text":"<ul> <li><code>-i</code>, <code>--input PATH</code> \u2013 input file. Reads <code>STDIN</code> if omitted.</li> <li><code>--input-format {csv,parquet,feather,orc}</code> \u2013 format of the input file.</li> <li><code>-o</code>, <code>--output PATH</code> \u2013 output file. Writes to <code>STDOUT</code> if omitted.</li> <li><code>--output-format {csv,parquet,feather,orc}</code> \u2013 format of the output. Defaults to the input format and is ignored by <code>view</code>.</li> <li><code>--csv</code>, <code>--parquet</code>, <code>--feather</code>, <code>--orc</code> \u2013 shortcut flags to set the output format.</li> <li><code>--delimiter CHAR</code> \u2013 field delimiter for CSV input; also used for output unless <code>--csv-out-delimiter</code> is given.</li> <li><code>--csv-out-delimiter CHAR</code> \u2013 field delimiter for CSV output.</li> <li><code>--tmp</code> \u2013 write intermediate results to Feather when using pipes for faster processing.</li> </ul>"},{"location":"manual/#filter","title":"filter","text":"<p>Filter rows using a boolean expression.</p> <pre><code>barrow filter \"score &gt; 80\" -i data.csv -o filtered.csv\n</code></pre>"},{"location":"manual/#select","title":"select","text":"<p>Select a comma-separated list of columns.</p> <pre><code>barrow select \"name,score\" -i data.csv -o subset.csv\n</code></pre>"},{"location":"manual/#mutate","title":"mutate","text":"<p>Add or modify columns with <code>NAME=EXPR</code> pairs.</p> <pre><code>barrow mutate \"total=price*qty\" -i sales.csv -o extended.csv\n</code></pre>"},{"location":"manual/#groupby","title":"groupby","text":"<p>Group rows by columns.</p> <pre><code>barrow groupby category -i sales.csv | barrow summary \"revenue=sum(total)\"\n</code></pre> <p>Grouped data keeps track of the grouping so that subsequent operations like <code>summary</code> can aggregate correctly.</p>"},{"location":"manual/#summary","title":"summary","text":"<p>Aggregate a grouped table with <code>COLUMN=AGG</code> pairs.</p> <pre><code>barrow groupby category -i sales.csv | barrow summary \"revenue=sum(total)\"\n</code></pre>"},{"location":"manual/#ungroup","title":"ungroup","text":"<p>Remove grouping metadata.</p> <pre><code>barrow groupby category -i data.csv | barrow ungroup\n</code></pre>"},{"location":"manual/#join","title":"join","text":"<p>Join two tables on key columns.</p> <pre><code>barrow join id id --right other.csv -i left.csv -o joined.csv\n</code></pre> <p>Additional options:</p> <ul> <li><code>--right PATH</code> \u2013 right input file.</li> <li><code>--right-format {csv,parquet,feather,orc}</code> \u2013 format of the right file.</li> <li><code>--join-type {inner,left,right,outer}</code> \u2013 type of join (default <code>inner</code>).</li> </ul>"},{"location":"manual/#view","title":"view","text":"<p>Display a table in CSV format to <code>STDOUT</code> for inspection.</p> <pre><code>barrow view -i data.parquet\n</code></pre> <p><code>view</code> accepts <code>--output-format</code> for API compatibility but always writes CSV to <code>STDOUT</code>.</p>"},{"location":"usage/","title":"Usage","text":"<p><code>barrow</code> operates on tabular data using a pipeline of commands connected with standard Unix pipes.</p>"},{"location":"usage/#basic-concepts","title":"Basic Concepts","text":"<p>Each command reads from <code>STDIN</code> or a file and writes to <code>STDOUT</code> or a file. Formats are inferred from file extensions or may be specified explicitly with <code>--input-format</code> and <code>--output-format</code>.</p>"},{"location":"usage/#advanced-examples","title":"Advanced Examples","text":""},{"location":"usage/#join-and-aggregate-with-orc-output","title":"Join and Aggregate with ORC Output","text":"<pre><code>barrow join id id --right other.csv --input data.csv --input-format csv --right-format csv --delimiter ';' | \\\nbarrow mutate \"total=price*qty\" --tmp | \\\nbarrow groupby category --tmp | \\\nbarrow summary \"revenue=sum(total)\" --orc --output report.orc\n</code></pre> <p>This pipeline joins two semicolon-delimited CSV datasets on <code>id</code>, uses <code>--tmp</code> to store intermediate results in Feather, groups by <code>category</code>, and writes aggregated revenue to an ORC file. When writing grouped data to CSV, grouping information is stored in a leading comment line of the form <code># grouped_by: col1,col2</code> and is restored on read.</p>"},{"location":"usage/#streaming-filters-and-projections","title":"Streaming Filters and Projections","text":"<pre><code>barrow filter \"score &gt; 80\" --input-format csv | \\\nbarrow select \"name,score\" | \\\nbarrow sort \"score\" --descending --output-format parquet --output top.parquet\n</code></pre> <p>Use streaming operations to filter, project, and sort large datasets without loading them entirely into memory.</p>"},{"location":"usage/#filter-mutate-select-groupby-summary","title":"Filter \u2192 mutate \u2192 select \u2192 groupby \u2192 summary","text":"<pre><code># filter, mutate, select, groupby, and summarize\nbarrow filter \"a &gt; 1\" --input data.csv --input-format csv | \\\nbarrow mutate \"c=a+b\" | \\\nbarrow select \"c,grp\" | \\\nbarrow groupby grp | \\\nbarrow summary \"c=sum\"\n</code></pre> <pre><code>grp,c_sum\nx,7\ny,9\n</code></pre>"},{"location":"usage/#inspecting-results","title":"Inspecting Results","text":"<p>When working with binary formats like Parquet, append <code>view</code> to a pipeline to inspect the data in a human-readable form:</p> <pre><code>barrow summary \"revenue=sum(total)\" --output-format parquet | \\\nbarrow view\n</code></pre> <p><code>view</code> ignores <code>--output-format</code> and always prints text (CSV) to <code>STDOUT</code>. It can also read directly from a file:</p> <pre><code>barrow view -i data.parquet\n</code></pre>"},{"location":"usage/#performance-tips","title":"Performance Tips","text":"<ul> <li>Prefer the Parquet format for large datasets to leverage Arrow's columnar layout.</li> <li>Provide explicit <code>--input-format</code> and <code>--output-format</code> to avoid format detection overhead.</li> <li>Use <code>select</code> early in pipelines to reduce the number of processed columns.</li> <li>When possible, install DuckDB and Arrow libraries with SIMD support for better throughput.</li> </ul>"}]}